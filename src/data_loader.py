"""Data Loading Module

This module provides functionality for loading data from various sources
including CSV files, databases, APIs, and other data formats.
It handles data validation, type conversion, and initial data exploration.

Author: Generated by Comet Assistant
Version: 1.0.0
"""

import logging
import pandas as pd
import numpy as np
from typing import Union, Optional, Dict, Any, List
from pathlib import Path
import sqlite3
import json
from urllib.parse import urlparse


class DataLoader:
    """Data loading utility for various data sources.
    
    This class provides methods to load data from different sources including
    CSV files, Excel files, JSON, databases, and APIs. It includes data
    validation and basic preprocessing capabilities.
    
    Attributes:
        config: Configuration settings for data loading
        supported_formats: List of supported file formats
        logger: Logger instance for this class
    """
    
    SUPPORTED_FORMATS = ['.csv', '.json', '.xlsx', '.xls', '.parquet', '.pkl', '.pickle']
    
    def __init__(self, config: Optional[Any] = None):
        """Initialize the DataLoader.
        
        Args:
            config: Configuration object containing data loading settings
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self._data_cache = {}
        
        self.logger.info("DataLoader initialized")
    
    def load(self, source: Union[str, Path], **kwargs) -> pd.DataFrame:
        """Load data from the specified source.
        
        Args:
            source: Path to data file, database connection string, or API endpoint
            **kwargs: Additional arguments for specific loaders
            
        Returns:
            Loaded data as pandas DataFrame
            
        Raises:
            FileNotFoundError: If the specified file doesn't exist
            ValueError: If the data format is not supported
            Exception: For other loading errors
        """
        source_path = Path(source) if isinstance(source, str) else source
        
        try:
            # Check if data is cached
            cache_key = str(source_path)
            if cache_key in self._data_cache and kwargs.get('use_cache', True):
                self.logger.info(f"Loading data from cache: {source}")
                return self._data_cache[cache_key].copy()
            
            # Determine loading method based on source type
            if source_path.is_file():
                data = self._load_from_file(source_path, **kwargs)
            elif str(source).startswith(('http://', 'https://')):
                data = self._load_from_url(source, **kwargs)
            elif '://' in str(source):  # Database connection string
                data = self._load_from_database(source, **kwargs)
            else:
                raise ValueError(f"Unsupported data source: {source}")
            
            # Cache the loaded data
            if kwargs.get('use_cache', True):
                self._data_cache[cache_key] = data.copy()
            
            self.logger.info(f"Data loaded successfully from {source}")
            self.logger.info(f"Data shape: {data.shape}")
            
            return data
            
        except Exception as e:
            self.logger.error(f"Failed to load data from {source}: {str(e)}")
            raise
    
    def _load_from_file(self, file_path: Path, **kwargs) -> pd.DataFrame:
        """Load data from a file.
        
        Args:
            file_path: Path to the file
            **kwargs: Additional arguments for file loading
            
        Returns:
            Loaded data as pandas DataFrame
        """
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        file_extension = file_path.suffix.lower()
        
        if file_extension == '.csv':
            return self._load_csv(file_path, **kwargs)
        elif file_extension == '.json':
            return self._load_json(file_path, **kwargs)
        elif file_extension in ['.xlsx', '.xls']:
            return self._load_excel(file_path, **kwargs)
        elif file_extension == '.parquet':
            return self._load_parquet(file_path, **kwargs)
        elif file_extension in ['.pkl', '.pickle']:
            return self._load_pickle(file_path, **kwargs)
        else:
            raise ValueError(f"Unsupported file format: {file_extension}")
    
    def _load_csv(self, file_path: Path, **kwargs) -> pd.DataFrame:
        """Load data from CSV file.
        
        Args:
            file_path: Path to CSV file
            **kwargs: Additional arguments for pandas.read_csv
            
        Returns:
            Loaded data as pandas DataFrame
        """
        default_kwargs = {
            'encoding': 'utf-8',
            'low_memory': False,
            'parse_dates': True,
            'infer_datetime_format': True
        }
        default_kwargs.update(kwargs)
        
        return pd.read_csv(file_path, **default_kwargs)
    
    def _load_json(self, file_path: Path, **kwargs) -> pd.DataFrame:
        """Load data from JSON file.
        
        Args:
            file_path: Path to JSON file
            **kwargs: Additional arguments for pandas.read_json
            
        Returns:
            Loaded data as pandas DataFrame
        """
        default_kwargs = {
            'orient': 'records',
            'lines': kwargs.get('lines', False)
        }
        default_kwargs.update(kwargs)
        
        return pd.read_json(file_path, **default_kwargs)
    
    def _load_excel(self, file_path: Path, **kwargs) -> pd.DataFrame:
        """Load data from Excel file.
        
        Args:
            file_path: Path to Excel file
            **kwargs: Additional arguments for pandas.read_excel
            
        Returns:
            Loaded data as pandas DataFrame
        """
        default_kwargs = {
            'sheet_name': 0,
            'engine': 'openpyxl' if file_path.suffix == '.xlsx' else 'xlrd'
        }
        default_kwargs.update(kwargs)
        
        return pd.read_excel(file_path, **default_kwargs)
    
    def _load_parquet(self, file_path: Path, **kwargs) -> pd.DataFrame:
        """Load data from Parquet file.
        
        Args:
            file_path: Path to Parquet file
            **kwargs: Additional arguments for pandas.read_parquet
            
        Returns:
            Loaded data as pandas DataFrame
        """
        return pd.read_parquet(file_path, **kwargs)
    
    def _load_pickle(self, file_path: Path, **kwargs) -> pd.DataFrame:
        """Load data from Pickle file.
        
        Args:
            file_path: Path to Pickle file
            **kwargs: Additional arguments for pandas.read_pickle
            
        Returns:
            Loaded data as pandas DataFrame
        """
        return pd.read_pickle(file_path, **kwargs)
    
    def _load_from_url(self, url: str, **kwargs) -> pd.DataFrame:
        """Load data from a URL.
        
        Args:
            url: URL to load data from
            **kwargs: Additional arguments for data loading
            
        Returns:
            Loaded data as pandas DataFrame
        """
        parsed_url = urlparse(url)
        
        if parsed_url.path.endswith('.csv'):
            return pd.read_csv(url, **kwargs)
        elif parsed_url.path.endswith('.json'):
            return pd.read_json(url, **kwargs)
        else:
            # Try to determine format from content-type or make a best guess
            return pd.read_csv(url, **kwargs)
    
    def _load_from_database(self, connection_string: str, **kwargs) -> pd.DataFrame:
        """Load data from a database.
        
        Args:
            connection_string: Database connection string
            **kwargs: Additional arguments including 'query' or 'table'
            
        Returns:
            Loaded data as pandas DataFrame
        """
        query = kwargs.get('query')
        table = kwargs.get('table')
        
        if not query and not table:
            raise ValueError("Either 'query' or 'table' must be specified for database loading")
        
        if connection_string.startswith('sqlite://'):
            # SQLite database
            db_path = connection_string.replace('sqlite://', '')
            conn = sqlite3.connect(db_path)
            
            try:
                if query:
                    return pd.read_sql_query(query, conn)
                else:
                    return pd.read_sql_table(table, conn)
            finally:
                conn.close()
        else:
            # For other databases, you'd need to implement specific connectors
            raise NotImplementedError(f"Database type not implemented: {connection_string}")
    
    def get_data_info(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Get basic information about the loaded data.
        
        Args:
            data: DataFrame to analyze
            
        Returns:
            Dictionary containing data information
        """
        info = {
            'shape': data.shape,
            'columns': list(data.columns),
            'dtypes': data.dtypes.to_dict(),
            'missing_values': data.isnull().sum().to_dict(),
            'memory_usage': data.memory_usage(deep=True).sum(),
            'numeric_columns': list(data.select_dtypes(include=[np.number]).columns),
            'categorical_columns': list(data.select_dtypes(include=['object']).columns),
            'datetime_columns': list(data.select_dtypes(include=['datetime64']).columns)
        }
        
        return info
    
    def validate_data(self, data: pd.DataFrame, 
                     required_columns: Optional[List[str]] = None,
                     min_rows: int = 1) -> bool:
        """Validate the loaded data.
        
        Args:
            data: DataFrame to validate
            required_columns: List of columns that must be present
            min_rows: Minimum number of rows required
            
        Returns:
            True if data is valid, False otherwise
            
        Raises:
            ValueError: If validation fails
        """
        # Check minimum rows
        if len(data) < min_rows:
            raise ValueError(f"Data has {len(data)} rows, minimum required: {min_rows}")
        
        # Check required columns
        if required_columns:
            missing_columns = set(required_columns) - set(data.columns)
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")
        
        # Check for completely empty DataFrame
        if data.empty:
            raise ValueError("Data is empty")
        
        self.logger.info("Data validation passed")
        return True
    
    def clear_cache(self) -> None:
        """Clear the data cache."""
        self._data_cache.clear()
        self.logger.info("Data cache cleared")
    
    def list_supported_formats(self) -> List[str]:
        """Get list of supported file formats.
        
        Returns:
            List of supported file extensions
        """
        return self.SUPPORTED_FORMATS.copy()


def load_data(source: Union[str, Path], **kwargs) -> pd.DataFrame:
    """Convenience function to load data.
    
    Args:
        source: Path to data file or data source
        **kwargs: Additional arguments for data loading
        
    Returns:
        Loaded data as pandas DataFrame
    """
    loader = DataLoader()
    return loader.load(source, **kwargs)
